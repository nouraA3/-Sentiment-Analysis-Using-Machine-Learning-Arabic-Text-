{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#note: lab 2 is under lab1"
      ],
      "metadata": {
        "id": "EVGl1nH8nwhp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OereUT7EP-_"
      },
      "source": [
        "# lab 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pnMnJXm6agv"
      },
      "source": [
        "# step 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v38MRleKnQnp",
        "outputId": "3941d497-3f6a-4d8f-c99c-5ee3ac9d87fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting camel-tools\n",
            "  Downloading camel_tools-1.5.5-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.17.0)\n",
            "Collecting docopt (from camel-tools)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from camel-tools) (5.5.1)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.13.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.6.1)\n",
            "Collecting dill (from camel-tools)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.5.1+cu124)\n",
            "Collecting transformers<4.44.0,>=4.0 (from camel-tools)\n",
            "  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.32.3)\n",
            "Collecting emoji (from camel-tools)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting pyrsistent (from camel-tools)\n",
            "  Downloading pyrsistent-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from camel-tools) (4.67.1)\n",
            "Collecting muddler (from camel-tools)\n",
            "  Downloading muddler-0.1.3-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting camel-kenlm>=2024.5.6 (from camel-tools)\n",
            "  Downloading camel-kenlm-2024.5.6.zip (556 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->camel-tools) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.5.2)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers<4.44.0,>=4.0->camel-tools)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->camel-tools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->camel-tools) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->camel-tools) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (2024.12.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->camel-tools) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->camel-tools) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->camel-tools) (3.0.2)\n",
            "Downloading camel_tools-1.5.5-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.5/124.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.43.4-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading muddler-0.1.3-py3-none-any.whl (16 kB)\n",
            "Downloading pyrsistent-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (120 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: camel-kenlm, docopt\n",
            "  Building wheel for camel-kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camel-kenlm: filename=camel_kenlm-2024.5.6-cp311-cp311-linux_x86_64.whl size=3186748 sha256=247a95447f34e7ced71a6b5bafe239e5847d86a3b6b3f93ee4c8b08d110d53d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/a6/a5/4f25e9750d0cddf53013b69f0e4d18930a2f89a78fc6019c97\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=45420c528c46bcbcc40da654170d18d7356561f3fa8846a318d5fc6609fde86d\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built camel-kenlm docopt\n",
            "Installing collected packages: docopt, camel-kenlm, pyrsistent, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, muddler, emoji, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, camel-tools\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.47.1\n",
            "    Uninstalling transformers-4.47.1:\n",
            "      Successfully uninstalled transformers-4.47.1\n",
            "Successfully installed camel-kenlm-2024.5.6 camel-tools-1.5.5 dill-0.3.9 docopt-0.6.2 emoji-2.14.1 muddler-0.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyrsistent-0.20.0 tokenizers-0.19.1 transformers-4.43.4\n",
            "Usage:\n",
            "    camel_data (-i | --install) [-f | --force] <PACKAGE>\n",
            "    camel_data (-p | --post-install) <PACKAGE> <ARGS>...\n",
            "    camel_data (-l | --list)\n",
            "    camel_data (-u | --update)\n",
            "    camel_data (-v | --version)\n",
            "    camel_data (-h | --help)\n",
            "Collecting farasapy\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from farasapy) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from farasapy) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->farasapy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->farasapy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->farasapy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->farasapy) (2024.12.14)\n",
            "Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#import Necessary Libraries\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "!pip install camel-tools\n",
        "!camel_data full\n",
        "!pip install -U farasapy\n",
        "\n",
        "\n",
        "from farasa.pos import FarasaPOSTagger\n",
        "from farasa.ner import FarasaNamedEntityRecognizer\n",
        "from farasa.diacratizer import FarasaDiacritizer\n",
        "from farasa.segmenter import FarasaSegmenter\n",
        "from farasa.stemmer import FarasaStemmer as stemmer\n",
        "\n",
        "# Import the dediac_ar function from camel_tools to remove diacritics (Tashkeel)\n",
        "from camel_tools.utils.dediac import dediac_ar\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Import the stopwords module\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNLU72YwzEC6"
      },
      "source": [
        "# step 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6rYieX-0nCP",
        "outputId": "6d365309-1b68-4912-f054-472fb43b786d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "  dialect  sarcasm sentiment original_sentiment  \\\n",
            "0    gulf    False  negative           negative   \n",
            "1     msa    False   neutral           positive   \n",
            "2   egypt    False   neutral            neutral   \n",
            "3  levant     True   neutral           negative   \n",
            "4     msa    False   neutral           negative   \n",
            "\n",
            "                                               tweet   source  \n",
            "0  \"Ù†ØµÙŠØ­Ù‡ Ù…Ø§ Ø¹Ù…Ø±Ùƒ Ø§ØªÙ†Ø²Ù„ Ù„Ø¹Ø¨Ø© Ø³ÙˆØ¨Ø± Ù…Ø§Ø±ÙŠÙˆ Ù…Ø´ Ø²ÙŠ Ù…Ø§ ...  semeval  \n",
            "1  \"#Ù†Ø§Ø¯ÙŠÙ†_Ù†Ø³ÙŠØ¨_Ù†Ø¬ÙŠÙ… â¤ï¸â¤ï¸â¤ï¸Ù…Ø¬Ù„Ø© #Ù…Ø§Ø±ÙŠ_ÙƒÙ„ÙŠØ± ğŸ’­#Ù…Ù„ÙƒØ©...  semeval  \n",
            "2                      \"@Alito_NBA Ø§ØªÙˆÙ‚Ø¹ Ø§Ù†Ù‡ Ø¨ÙŠØ³ØªÙ…Ø±\"  semeval  \n",
            "3     \"@KSA24 ÙŠØ¹Ù†ÙŠ \"Ø¨Ù…ÙˆØ§ÙÙ‚ØªÙ†Ø§\" Ù„Ø£Ù† Ø¯Ù…Ø´Ù‚ ØµØ§ÙŠØ±Ø© Ù…ÙˆØ³ÙƒÙˆ\"  semeval  \n",
            "4  \"RT @alaahmad20: Ù‚Ø§Ø¦Ø¯ ÙÙŠ Ø§Ù„Ø­Ø±Ø³ ÙŠØ¹ØªØ±Ù Ø¨ÙÙ‚Ø¯Ø§Ù† Ø§Ù„...  semeval  \n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8437 entries, 0 to 8436\n",
            "Data columns (total 6 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   dialect             8437 non-null   object\n",
            " 1   sarcasm             8437 non-null   bool  \n",
            " 2   sentiment           8437 non-null   object\n",
            " 3   original_sentiment  8437 non-null   object\n",
            " 4   tweet               8437 non-null   object\n",
            " 5   source              8437 non-null   object\n",
            "dtypes: bool(1), object(5)\n",
            "memory usage: 337.9+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('/content/ArSarcasm_train.csv')\n",
        "# Inspect the dataset\n",
        "print(\"Original Dataset:\")\n",
        "print(df.head()) #shows the fisrt 5 rows\n",
        "print(\"Dataset Info:\")\n",
        "print(df.info()) #shows the data information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCgt1_Dx18pK"
      },
      "source": [
        "# step 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B6X7gjk1yqf",
        "outputId": "7d52f021-0a20-4e8e-acdc-8fc5083be54c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Normalization:\n",
            "0    Ù†ØµÙŠØ­Ù‡ Ù…Ø§ Ø¹Ù…Ø±Ùƒ Ø§ØªÙ†Ø²Ù„ Ù„Ø¹Ø¨Ø© Ø³ÙˆØ¨Ø± Ù…Ø§Ø±ÙŠÙˆ Ù…Ø´ Ø²ÙŠ Ù…Ø§ Ùƒ...\n",
            "1          Ù†Ø§Ø¯ÙŠÙ† Ù†Ø³ÙŠØ¨ Ù†Ø¬ÙŠÙ… Ù…Ø¬Ù„Ø© Ù…Ø§Ø±ÙŠ ÙƒÙ„ÙŠØ± Ù…Ù„ÙƒØ© Ø§Ù„ØµØ­Ø±Ø§Ø¡\n",
            "2                                     Ø§ØªÙˆÙ‚Ø¹ Ø§Ù†Ù‡ Ø¨ÙŠØ³ØªÙ…Ø±\n",
            "3                  ÙŠØ¹Ù†ÙŠ Ø¨Ù…ÙˆØ§ÙÙ‚ØªÙ†Ø§ Ù„Ø£Ù† Ø¯Ù…Ø´Ù‚ ØµØ§ÙŠØ±Ø© Ù…ÙˆØ³ÙƒÙˆ\n",
            "4    Ù‚Ø§Ø¦Ø¯ ÙÙŠ Ø§Ù„Ø­Ø±Ø³ ÙŠØ¹ØªØ±Ù Ø¨ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ø³ÙŠØ·Ø±Ø© Ø§Ù„Ø£Ù…Ù†ÙŠØ© ÙÙŠ ...\n",
            "Name: tweet, dtype: object\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def normalize_text(text):\n",
        "    # Replace underscore with a space\n",
        "    text = text.replace('_', ' ')\n",
        "    # Remove non-Arabic characters\n",
        "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
        "    # Remove Tatweel\n",
        "    text = text.replace('Ù€', '')\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "#  Apply normalization to the 'tweet' column\n",
        "df['tweet'] = df['tweet'].apply(normalize_text)\n",
        "print(\"After Normalization:\")\n",
        "print(df['tweet'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GJO7GFt2eFs"
      },
      "source": [
        "# step 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQp8QaHT2flM",
        "outputId": "7f788770-5dbd-46cb-d426-548b3a52e031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Removing Diacritics:\n",
            "0    Ù†ØµÙŠØ­Ù‡ Ù…Ø§ Ø¹Ù…Ø±Ùƒ Ø§ØªÙ†Ø²Ù„ Ù„Ø¹Ø¨Ø© Ø³ÙˆØ¨Ø± Ù…Ø§Ø±ÙŠÙˆ Ù…Ø´ Ø²ÙŠ Ù…Ø§ Ùƒ...\n",
            "1          Ù†Ø§Ø¯ÙŠÙ† Ù†Ø³ÙŠØ¨ Ù†Ø¬ÙŠÙ… Ù…Ø¬Ù„Ø© Ù…Ø§Ø±ÙŠ ÙƒÙ„ÙŠØ± Ù…Ù„ÙƒØ© Ø§Ù„ØµØ­Ø±Ø§Ø¡\n",
            "2                                     Ø§ØªÙˆÙ‚Ø¹ Ø§Ù†Ù‡ Ø¨ÙŠØ³ØªÙ…Ø±\n",
            "3                  ÙŠØ¹Ù†ÙŠ Ø¨Ù…ÙˆØ§ÙÙ‚ØªÙ†Ø§ Ù„Ø£Ù† Ø¯Ù…Ø´Ù‚ ØµØ§ÙŠØ±Ø© Ù…ÙˆØ³ÙƒÙˆ\n",
            "4    Ù‚Ø§Ø¦Ø¯ ÙÙŠ Ø§Ù„Ø­Ø±Ø³ ÙŠØ¹ØªØ±Ù Ø¨ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ø³ÙŠØ·Ø±Ø© Ø§Ù„Ø£Ù…Ù†ÙŠØ© ÙÙŠ ...\n",
            "Name: tweet, dtype: object\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define a function to remove diacritics from Arabic text\n",
        "def remove_diacritics(text):\n",
        " # Use dediac_ar to strip diacritics from the input text\n",
        "    return dediac_ar(text)\n",
        "\n",
        "# Apply the diacritic removal function to the 'tweet' column in the dataset\n",
        "df['tweet'] = df['tweet'].apply(remove_diacritics)\n",
        "\n",
        "# Print the first few rows of the 'tweet' column after removing diacritics\n",
        "print(\"After Removing Diacritics:\")\n",
        "print(df['tweet'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uih9jufu2tGr"
      },
      "source": [
        "# step 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HonffwcrGzr7",
        "outputId": "cb572018-139e-4cf8-e624-30604d5a13b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Removing Stopwords:\n",
            "0    Ù†ØµÙŠØ­Ù‡ Ø¹Ù…Ø±Ùƒ Ø§ØªÙ†Ø²Ù„ Ù„Ø¹Ø¨Ø© Ø³ÙˆØ¨Ø± Ù…Ø§Ø±ÙŠÙˆ Ù…Ø´ Ø²ÙŠ ÙƒÙ†Ø§ Ù…ØªÙˆ...\n",
            "1          Ù†Ø§Ø¯ÙŠÙ† Ù†Ø³ÙŠØ¨ Ù†Ø¬ÙŠÙ… Ù…Ø¬Ù„Ø© Ù…Ø§Ø±ÙŠ ÙƒÙ„ÙŠØ± Ù…Ù„ÙƒØ© Ø§Ù„ØµØ­Ø±Ø§Ø¡\n",
            "2                                     Ø§ØªÙˆÙ‚Ø¹ Ø§Ù†Ù‡ Ø¨ÙŠØ³ØªÙ…Ø±\n",
            "3                  ÙŠØ¹Ù†ÙŠ Ø¨Ù…ÙˆØ§ÙÙ‚ØªÙ†Ø§ Ù„Ø£Ù† Ø¯Ù…Ø´Ù‚ ØµØ§ÙŠØ±Ø© Ù…ÙˆØ³ÙƒÙˆ\n",
            "4    Ù‚Ø§Ø¦Ø¯ Ø§Ù„Ø­Ø±Ø³ ÙŠØ¹ØªØ±Ù Ø¨ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ø³ÙŠØ·Ø±Ø© Ø§Ù„Ø£Ù…Ù†ÙŠØ© Ø´Ø±Ù‚ÙŠ Ùˆ...\n",
            "Name: tweet, dtype: object\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Load Arabic stopwords from NLTK\n",
        "arabic_stopwords = set(stopwords.words('arabic'))\n",
        "\n",
        "# Define the stopword removal function\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()  # Split the text into words\n",
        "    filtered_words = [word for word in words if word not in arabic_stopwords]  # Filter stopwords\n",
        "    return ' '.join(filtered_words)  # Rejoin the filtered words into a string\n",
        "\n",
        "# Apply the function to the 'tweet' column\n",
        "df['tweet'] = df['tweet'].apply(remove_stopwords)\n",
        "\n",
        "# Display the cleaned text\n",
        "print(\"After Removing Stopwords:\")\n",
        "print(df['tweet'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaY2uWXi6o3-"
      },
      "source": [
        "# test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRLHlXEKBxzK"
      },
      "source": [
        "# step 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "eAuZ4GkL6rbQ"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "\n",
        "df1 = pd.read_csv('/content/ArSarcasm_test.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICET7zYn8e9t",
        "outputId": "b6503d57-1dde-4867-e902-429f40cb1bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "  dialect  sarcasm sentiment original_sentiment  \\\n",
            "0     msa     True  negative           negative   \n",
            "1    gulf    False  positive            neutral   \n",
            "2     msa     True   neutral            neutral   \n",
            "3     msa    False   neutral            neutral   \n",
            "4     msa    False   neutral            neutral   \n",
            "\n",
            "                                               tweet   source  \n",
            "0  \"@AbuEmad74241481 @Cesars2014 Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡ Ø­Ø²Ø¨ Ø§Ù„Ù„Ù‡ ...  semeval  \n",
            "1  \"RT @JannetForster: Ø§Ù„Ø¨Ù†Ø§Øª Ø§Ù„Ù„ÙŠ Ù… ØµØ§Ù…Ùˆ Ø¨Ù‚ÙˆÙ„ÙƒÙ… ...  semeval  \n",
            "2             Ø§Ø´Ø§Ø±Ø© Ø±Ø§Ø¨Ø¹Ø© Ø§Ø´Ø¨Ù‡ Ø¨Ù†Ø§Ø± ØªØ­Ø±Ù‚ Ø§Ù„Ø§Ù†Ù‚Ù„Ø§Ø¨ÙŠÙŠÙ†     astd  \n",
            "3  \"@EGYPTAIR Ù…Ø§Ù‡ÙŠ Ù…Ù…ÙŠØ²Ø§Øª Ø¯Ø±Ø¬Ù‡ Ø¨Ø²Ù†Ø³ Ø¹Ù„Ù…Ø§Ù‹ Ø§Ù†ÙŠ ÙÙŠ ...  semeval  \n",
            "4  Ù…Ø§ Ù„Ø§ ØªØ±Ø§Ù‡ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ„ÙØ§Ø² Ù…Ù†Ø§ÙØ³Ø© Ø´Ø¯ÙŠØ¯Ø© Ø¨ÙŠÙ† Ø§Ù„Ù…Ø±Ø´Ø­...  semeval  \n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2110 entries, 0 to 2109\n",
            "Data columns (total 6 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   dialect             2110 non-null   object\n",
            " 1   sarcasm             2110 non-null   bool  \n",
            " 2   sentiment           2110 non-null   object\n",
            " 3   original_sentiment  2110 non-null   object\n",
            " 4   tweet               2110 non-null   object\n",
            " 5   source              2110 non-null   object\n",
            "dtypes: bool(1), object(5)\n",
            "memory usage: 84.6+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Inspect the dataset\n",
        "print(\"Original Dataset:\")\n",
        "print(df1.head()) #shows the fisrt 5 rows\n",
        "print(\"Dataset Info:\")\n",
        "print(df1.info()) #shows the data information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb8CDv4Y9EaD"
      },
      "source": [
        "# step 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JGgqJXU9F0H",
        "outputId": "8b1956a8-eceb-4eb3-b21d-d87027f56175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Normalization:\n",
            "0    Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡ Ø­Ø²Ø¨ Ø§Ù„Ù„Ù‡ Ø­Ø·Ù…ÙˆØ§ Ø§Ø³Ø·ÙˆØ±Ø© Ø§Ù„Ù…ÙŠØ±ÙƒØ§ÙØ§ Ø§Ù„Ø§Ø³Ø±Ø§Ø¦...\n",
            "1    Ø§Ù„Ø¨Ù†Ø§Øª Ø§Ù„Ù„ÙŠ Ù… ØµØ§Ù…Ùˆ Ø¨Ù‚ÙˆÙ„ÙƒÙ… ØªØ±Ø§ Ø±Ù…Ø¶Ø§Ù† Ù‚Ø±Ø¨ Ùˆ Ø§Ù„ÙˆÙ‚...\n",
            "2               Ø§Ø´Ø§Ø±Ø© Ø±Ø§Ø¨Ø¹Ø© Ø§Ø´Ø¨Ù‡ Ø¨Ù†Ø§Ø± ØªØ­Ø±Ù‚ Ø§Ù„Ø§Ù†Ù‚Ù„Ø§Ø¨ÙŠÙŠÙ†\n",
            "3    Ù…Ø§Ù‡ÙŠ Ù…Ù…ÙŠØ²Ø§Øª Ø¯Ø±Ø¬Ù‡ Ø¨Ø²Ù†Ø³ Ø¹Ù„Ù…Ø§Ù‹ Ø§Ù†ÙŠ ÙÙŠ Ø¨Ø±ÙˆÙƒØ³ÙŠÙ„ Ùˆ Ø§...\n",
            "4    Ù…Ø§ Ù„Ø§ ØªØ±Ø§Ù‡ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ„ÙØ§Ø² Ù…Ù†Ø§ÙØ³Ø© Ø´Ø¯ÙŠØ¯Ø© Ø¨ÙŠÙ† Ø§Ù„Ù…Ø±Ø´Ø­...\n",
            "Name: tweet, dtype: object\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def normalize_text(text):\n",
        "    # Replace underscore with a space\n",
        "    text = text.replace('_', ' ')\n",
        "    # Remove non-Arabic characters\n",
        "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
        "    # Remove Tatweel\n",
        "    text = text.replace('Ù€', '')\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Apply normalization to the 'tweet' column\n",
        "df1['tweet'] = df1['tweet'].apply(normalize_text)\n",
        "print(\"After Normalization:\")\n",
        "print(df1['tweet'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11xnjmxA9YRO"
      },
      "source": [
        "# step 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xc2b_C_B9ZST",
        "outputId": "6acc46b4-707d-4a00-e1b8-6080a21b7128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Removing Diacritics:\n",
            "0    Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡ Ø­Ø²Ø¨ Ø§Ù„Ù„Ù‡ Ø­Ø·Ù…ÙˆØ§ Ø§Ø³Ø·ÙˆØ±Ø© Ø§Ù„Ù…ÙŠØ±ÙƒØ§ÙØ§ Ø§Ù„Ø§Ø³Ø±Ø§Ø¦...\n",
            "1    Ø§Ù„Ø¨Ù†Ø§Øª Ø§Ù„Ù„ÙŠ Ù… ØµØ§Ù…Ùˆ Ø¨Ù‚ÙˆÙ„ÙƒÙ… ØªØ±Ø§ Ø±Ù…Ø¶Ø§Ù† Ù‚Ø±Ø¨ Ùˆ Ø§Ù„ÙˆÙ‚...\n",
            "2               Ø§Ø´Ø§Ø±Ø© Ø±Ø§Ø¨Ø¹Ø© Ø§Ø´Ø¨Ù‡ Ø¨Ù†Ø§Ø± ØªØ­Ø±Ù‚ Ø§Ù„Ø§Ù†Ù‚Ù„Ø§Ø¨ÙŠÙŠÙ†\n",
            "3    Ù…Ø§Ù‡ÙŠ Ù…Ù…ÙŠØ²Ø§Øª Ø¯Ø±Ø¬Ù‡ Ø¨Ø²Ù†Ø³ Ø¹Ù„Ù…Ø§ Ø§Ù†ÙŠ ÙÙŠ Ø¨Ø±ÙˆÙƒØ³ÙŠÙ„ Ùˆ Ø§Ø±...\n",
            "4    Ù…Ø§ Ù„Ø§ ØªØ±Ø§Ù‡ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ„ÙØ§Ø² Ù…Ù†Ø§ÙØ³Ø© Ø´Ø¯ÙŠØ¯Ø© Ø¨ÙŠÙ† Ø§Ù„Ù…Ø±Ø´Ø­...\n",
            "Name: tweet, dtype: object\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define a function to remove diacritics from Arabic text\n",
        "def remove_diacritics(text):\n",
        " # Use dediac_ar to strip diacritics from the input text\n",
        "    return dediac_ar(text)\n",
        "\n",
        "# Apply the diacritic removal function to the 'tweet' column in the dataset\n",
        "df1['tweet'] = df1['tweet'].apply(remove_diacritics)\n",
        "\n",
        "# Print the first few rows of the 'tweet' column after removing diacritics\n",
        "print(\"After Removing Diacritics:\")\n",
        "print(df1['tweet'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xt3CPNg9p3s"
      },
      "source": [
        "# step 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FBsK8yD9s63",
        "outputId": "e19d2e63-cb2a-4334-c4ff-02c8dd4b3977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Removing Stopwords:\n",
            "0    Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡ Ø­Ø²Ø¨ Ø§Ù„Ù„Ù‡ Ø­Ø·Ù…ÙˆØ§ Ø§Ø³Ø·ÙˆØ±Ø© Ø§Ù„Ù…ÙŠØ±ÙƒØ§ÙØ§ Ø§Ù„Ø§Ø³Ø±Ø§Ø¦...\n",
            "1    Ø§Ù„Ø¨Ù†Ø§Øª Ø§Ù„Ù„ÙŠ ØµØ§Ù…Ùˆ Ø¨Ù‚ÙˆÙ„ÙƒÙ… ØªØ±Ø§ Ø±Ù…Ø¶Ø§Ù† Ù‚Ø±Ø¨ Ø§Ù„ÙˆÙ‚Øª Ù‚Ù„...\n",
            "2               Ø§Ø´Ø§Ø±Ø© Ø±Ø§Ø¨Ø¹Ø© Ø§Ø´Ø¨Ù‡ Ø¨Ù†Ø§Ø± ØªØ­Ø±Ù‚ Ø§Ù„Ø§Ù†Ù‚Ù„Ø§Ø¨ÙŠÙŠÙ†\n",
            "3    Ù…Ø§Ù‡ÙŠ Ù…Ù…ÙŠØ²Ø§Øª Ø¯Ø±Ø¬Ù‡ Ø¨Ø²Ù†Ø³ Ø¹Ù„Ù…Ø§ Ø§Ù†ÙŠ Ø¨Ø±ÙˆÙƒØ³ÙŠÙ„ Ø§Ø±ÙŠØ¯ Ø§Ù„...\n",
            "4    ØªØ±Ø§Ù‡ Ø§Ù„ØªÙ„ÙØ§Ø² Ù…Ù†Ø§ÙØ³Ø© Ø´Ø¯ÙŠØ¯Ø© Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ù‡ÙŠÙ„Ø§Ø±ÙŠ ÙƒÙ„Ù†Øª...\n",
            "Name: tweet, dtype: object\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load Arabic stopwords from NLTK\n",
        "arabic_stopwords = set(stopwords.words('arabic'))\n",
        "\n",
        "# Define the stopword removal function\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()  # Split the text into words\n",
        "    filtered_words = [word for word in words if word not in arabic_stopwords]  # Filter stopwords\n",
        "    return ' '.join(filtered_words)  # Rejoin the filtered words into a string\n",
        "\n",
        "# Apply the function to the 'tweet' column\n",
        "df1['tweet'] = df1['tweet'].apply(remove_stopwords)\n",
        "\n",
        "# Display the cleaned text\n",
        "print(\"After Removing Stopwords:\")\n",
        "print(df1['tweet'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK6zbLz14T-I"
      },
      "source": [
        "# lab2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wicFiX1EfxU"
      },
      "source": [
        "#step 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "7r8tA6K76i8k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "nD3kU7P-60bB"
      },
      "outputs": [],
      "source": [
        "# Load the datasets\n",
        "df = pd.read_csv('/content/ArSarcasm_train.csv') #train set\n",
        "df1 = pd.read_csv('/content/ArSarcasm_test.csv') #test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd6_SI7f7kcX",
        "outputId": "f52ff567-7d35-4b76-a47f-e361ad640986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names: ['dialect', 'sarcasm', 'sentiment', 'original_sentiment', 'tweet', 'source']\n",
            "Column names: ['dialect', 'sarcasm', 'sentiment', 'original_sentiment', 'tweet', 'source']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Display column names for train dataset\n",
        "print(\"Column names:\", df.columns.tolist())\n",
        "# Display column names for test dataset\n",
        "print(\"Column names:\", df1.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "PGxIwoIQ9IYK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Manually split the data into training and testing sets\n",
        "X_train = df['tweet']\n",
        "X_test = df1['tweet']\n",
        "y_train = df['sentiment']\n",
        "y_test = df1['sentiment']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcYAO-vT9Yre",
        "outputId": "1ce7fe97-7294-4770-fcac-947919ab1ba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (8437,)\n",
            "Shape of X_test: (2110,)\n",
            "Shape of y_train: (8437,)\n",
            "Shape of y_test: (2110,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Print dataset shapes\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fVQmbOhEr8Z"
      },
      "source": [
        "#step 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "x5fs2jRq9ep6"
      },
      "outputs": [],
      "source": [
        " #Step 2: Feature Extraction using TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=10000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqxsw4jLEvzV"
      },
      "source": [
        "# step 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "uF48ZMDi9v7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8dc9d34-d793-413b-bf51-c36253f9bb9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 0.6123222748815166\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.56      0.58      0.57       716\n",
            "     neutral       0.64      0.80      0.71      1078\n",
            "    positive       0.74      0.04      0.08       316\n",
            "\n",
            "    accuracy                           0.61      2110\n",
            "   macro avg       0.64      0.47      0.45      2110\n",
            "weighted avg       0.63      0.61      0.57      2110\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Train and Evaluate Models using Random Forest\n",
        "model = RandomForestClassifier(n_estimators=300, max_depth=50, random_state=42)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model accuracy:\", accuracy)\n",
        "\n",
        "# Generate a detailed classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8fjtdntFdIK"
      },
      "source": [
        "seeing if there imbalence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8ZuNp2Y-Mmh",
        "outputId": "0e9af918-c53b-41bd-fde7-a5c52f09d450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiment\n",
            "neutral     4262\n",
            "negative    2813\n",
            "positive    1362\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df['sentiment'].value_counts())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}